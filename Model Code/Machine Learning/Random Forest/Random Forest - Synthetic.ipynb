{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, average_precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = pd.read_csv(\"/kaggle/input/combined-dataset-before-smote/output-2.csv\", header = None, delimiter = ',')\n",
    "\n",
    "X = dataset.drop(columns = [26,27,28,29])\n",
    "y = dataset.filter([26,27,28,29])\n",
    "print(X,y)\n",
    "\n",
    "unique_rows = np.unique(y, axis=0).tolist()\n",
    "print(unique_rows)\n",
    "\n",
    "y['combined']= y.values.tolist()\n",
    "y = y.drop(columns = [26,27,28,29])\n",
    "print(y)\n",
    "print(y.shape)\n",
    "#merges mutliple targets into single multiclassfication for y\n",
    "i = 0\n",
    "while i < len(y):\n",
    "    j = 0\n",
    "    while j < len(unique_rows):\n",
    "        if y.loc[y.index[i],'combined'] == unique_rows[j]:\n",
    "            y.loc[y.index[i],'combined'] = unique_rows.index(unique_rows[j])\n",
    "        j +=1\n",
    "    i+=1\n",
    "    \n",
    "y['combined'] = y['combined'].astype(int)\n",
    "\n",
    "y.to_numpy()\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"done\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Convert y to 1-dimensional array\n",
    "y_labels = np.argmax(y.values, axis=1)\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "max_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                for max_features in param_grid['max_features']:\n",
    "                    params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf,\n",
    "                        'max_features': max_features\n",
    "                    }\n",
    "                    forest = RandomForestClassifier(**params)\n",
    "                    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                    scores = cross_val_score(forest, x_train, y_train, cv=skf)\n",
    "                    score_mean = scores.mean()\n",
    "                    if score_mean > max_score:\n",
    "                        max_score = score_mean\n",
    "                        best_params = params\n",
    "\n",
    "print(\"Best Parameters for first dataset:\")\n",
    "print(best_params)\n",
    "print(\"Average Accuracy:\", max_score)\n",
    "f1_score_final = 0.0\n",
    "prc_auc_final = 0.0\n",
    "roc_auc_final = 0.0\n",
    "y_test_all = []\n",
    "y_pred_proba_all = []\n",
    "\n",
    "for train_index, test_index in skf.split(x_train, y_train):\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[test_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    y_train_fold = np.ravel(y_train_fold)\n",
    "    \n",
    "    final_forest = RandomForestClassifier(**best_params)\n",
    "    final_forest.fit(x_train_fold, y_train_fold)\n",
    "    y_pred_final = final_forest.predict_proba(x_val_fold)[:, 1]  # Predict probabilities\n",
    "    \n",
    "    y_test_all.extend(y_val_fold)\n",
    "    y_pred_proba_all.extend(y_pred_final)\n",
    "    \n",
    "    f1_final = f1_score(y_val_fold, (y_pred_final > 0.5).astype(int))\n",
    "    f1_score_final += f1_final\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_val_fold.ravel(), y_pred_final.ravel())\n",
    "    prc_auc = average_precision_score(y_val_fold.ravel(), y_pred_final.ravel())\n",
    "    prc_auc_final += prc_auc\n",
    "    \n",
    "fpr_final, tpr_final, _ = roc_curve(y_test_all, y_pred_proba_all)\n",
    "roc_auc_final = roc_auc_score(y_test_all, y_pred_proba_all)\n",
    "\n",
    "f1_score_final /= skf.n_splits\n",
    "prc_auc_final /= skf.n_splits\n",
    "\n",
    "conf_matrix_final = confusion_matrix(y_test_all, (np.array(y_pred_proba_all) > 0.5).astype(int))\n",
    "\n",
    "tpr_final_value = conf_matrix_final[1, 1] / (conf_matrix_final[1, 1] + conf_matrix_final[1, 0])\n",
    "fpr_final_value = conf_matrix_final[0, 1] / (conf_matrix_final[0, 1] + conf_matrix_final[0, 0])\n",
    "\n",
    "print(\"F1 Score:\", f1_score_final)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision-Recall AUC:\", prc_auc_final)\n",
    "print(\"ROC AUC:\", roc_auc_final)\n",
    "print(\"TPR for roc:\", tpr_final)\n",
    "print(\"FPR for roc:\", fpr_final)\n",
    "print(\"TPR:\", tpr_final_value)\n",
    "print(\"FPR:\", fpr_final_value)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_final, tpr_final, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc_final)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('/kaggle/working/roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (AUC = %0.2f)' % prc_auc_final)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('/kaggle/working/precision_recall_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"Average F1 Score\": f1_score_final, \n",
    "    \"Precision\": precision.tolist(),\n",
    "    \"Recall\": recall.tolist(), \n",
    "    \"Average True Positive Rate\": tpr_final_value, \n",
    "    \"Average False Positive Rate\": fpr_final_value, \n",
    "    \"Average ROC AUC\": roc_auc_final, \n",
    "    \"Average PRC AUC\": prc_auc_final,\n",
    "    \"TPR Array\" : tpr_final.tolist(),\n",
    "    \"FPR Array\" : fpr_final.tolist(),\n",
    "}\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/kaggle/working/Random Forest - Synthetic Dataset.json\"\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
