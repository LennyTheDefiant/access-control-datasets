{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/kaggle/input/amazon-kaggle-smote-official/Amazon_Kaggle_SMOTE.csv')\n",
    "\n",
    "X = df.iloc[:, :-1]  # Features\n",
    "y = df.iloc[:, -1]   # Target variable\n",
    "\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "n_splits = 5  # For example, 5 folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results from each fold\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "tprs = []\n",
    "fprs = []\n",
    "cms = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "roc_aucs = []\n",
    "prc_aucs = []\n",
    "\n",
    "# To store FPR, TPR, Recall, Precision arrays\n",
    "fpr_lists = []\n",
    "tpr_lists = []\n",
    "recall_lists = []\n",
    "precision_lists = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Logistic Regression Model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Calculate and append metrics\n",
    "    f1_scores.append(f1_score(y_test, predictions, average='macro'))\n",
    "    precisions.append(precision_score(y_test, predictions, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions, average='macro'))\n",
    "\n",
    "    # Confusion Matrix and AUC metrics\n",
    "    fpr_array, tpr_array, _ = roc_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "    roc_aucs.append(auc(fpr_array, tpr_array))\n",
    "    precision_array, recall_array, _ = precision_recall_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "    prc_aucs.append(auc(recall_array, precision_array))\n",
    "    \n",
    "    # Calculate mean FPR and TPR for the fold and append\n",
    "    fprs.append(np.mean(fpr_array))\n",
    "    tprs.append(np.mean(tpr_array))\n",
    "\n",
    "    # Store arrays for each fold\n",
    "    fpr_lists.append(fpr_array.tolist())\n",
    "    tpr_lists.append(tpr_array.tolist())\n",
    "    recall_lists.append(recall_array.tolist())\n",
    "    precision_lists.append(precision_array.tolist())\n",
    "\n",
    "     \n",
    "\n",
    "# Compile metrics into a dictionary\n",
    "metrics = {\n",
    "    'Average F1 Score': np.mean(f1_scores),\n",
    "    'Average Precision': np.mean(precisions),\n",
    "    'Average Recall': np.mean(recalls),\n",
    "    'Average True Positive Rate': np.mean(tprs),\n",
    "    'Average False Positive Rate': np.mean(fprs),\n",
    "    'Average ROC AUC': np.mean(roc_aucs),\n",
    "    'Average PRC AUC': np.mean(prc_aucs),\n",
    "    'Average Accuracy': np.mean(accuracies),\n",
    "    'FPR Lists': fpr_lists,\n",
    "    'TPR Lists': tpr_lists,\n",
    "    'Recall Lists': recall_lists,\n",
    "    'Precision Lists': precision_lists\n",
    "}\n",
    "\n",
    "# Save metrics to a JSON file\n",
    "with open('logistic_regression_evaluation_metrics_real_life_SMOTE.json', 'w') as file:\n",
    "    json.dump(metrics, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize ROC and PRC for the last fold\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fpr_array, tpr_array, label=f\"ROC curve (area = {roc_aucs[-1]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('logistic_regression_roc_curve_real_life_SMOTE.jpg')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall_array, precision_array, label=f\"PRC curve (area = {prc_aucs[-1]:.2f})\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('logistic_regression_prc_curve_real_life_SMOTE.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "dump(model, 'logistic_regression_model_real_life_SMOTE.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
