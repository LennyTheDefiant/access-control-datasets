{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_curve, confusion_matrix, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset from CSV\n",
    "dataFileName = '/kaggle/input/concatenated-pca/concatenated_data_synthethic_PCA.csv'  # Adjust path as necessary\n",
    "\n",
    "try:\n",
    "    raw_dataset = np.genfromtxt(dataFileName, delimiter=',')\n",
    "    dataset = raw_dataset[:, :]  # Adjust indexing based on your CSV structure\n",
    "    print(dataset.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load or parse the CSV file. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split features and labels\n",
    "metadata = train_dataset.shape[1] - 4\n",
    "x_train = train_dataset[:, :metadata]\n",
    "x_val = val_dataset[:, :metadata]\n",
    "y_train = train_dataset[:, metadata:].astype(int)\n",
    "y_val = val_dataset[:, metadata:].astype(int)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "# Reshape data to match the input shape of the model (batch size, height, width, channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1, 1)\n",
    "\n",
    "# Define a modified AlexNet model with adjusted pooling size\n",
    "def alexnet_model(input_shape, num_classes=4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(96, (3, 3), strides=(2, 2), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 1))(x)  # Adjusted pooling size\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 1))(x)  # Adjusted pooling size\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Model setup\n",
    "input_shape = x_train.shape[1:]\n",
    "model = alexnet_model(input_shape=input_shape)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='alexnet_best_synthetic_PCA.h5', verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "epochs = 30\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=epochs, validation_data=(x_val, y_val), shuffle=True, callbacks=callbacks)\n",
    "\n",
    "# Save the final model and training history\n",
    "model.save('alexnet_final_model_synthetic_PCA.h5')\n",
    "with open('alexnet_training_history_synthetic_PCA.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "scores = model.evaluate(x_val, y_val, verbose=1)\n",
    "y_preds = (model.predict(x_val) > 0.5).astype(int)\n",
    "y_pred_binary = (y_preds > 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_val.ravel(), y_preds.ravel(), average='macro')\n",
    "tn, fp, fn, tp = confusion_matrix(y_val.ravel(), y_pred_binary.ravel()).ravel()\n",
    "tpr1 = tp / (tp + fn)\n",
    "fpr1 = fp / (fp + tn)\n",
    "cm = confusion_matrix(y_val.ravel(), y_preds.ravel())\n",
    "\n",
    "# Calculate ROC and Precision-Recall curves\n",
    "fpr, tpr, _ = roc_curve(y_val.ravel(), y_preds.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "precision, recall, _ = precision_recall_curve(y_val.ravel(), y_preds.ravel())\n",
    "prc_auc = auc(recall, precision)\n",
    "\n",
    "# Save metrics and curves\n",
    "metrics = {\n",
    "    'Validation Loss': scores[0],\n",
    "    'Validation Accuracy': scores[1],\n",
    "    'F1 Score': f1,\n",
    "    'True Positive Rate' : tpr1,\n",
    "    'False Positive Rate' : fpr1,\n",
    "    'Confusion Matrix': cm.tolist(),\n",
    "    'ROC AUC': roc_auc,\n",
    "    'PRC AUC': prc_auc,\n",
    "    'FPR Array': fpr.tolist(),\n",
    "    'TPR Array': tpr.tolist(),\n",
    "    'Precision Array': precision.tolist(),\n",
    "    'Recall Array': recall.tolist()\n",
    "}\n",
    "with open('alexnet_evaluation_metrics_synthetic_PCA.json', 'w') as file:\n",
    "    json.dump(metrics, file)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('alexnet_roc_curve_synthetic_PCA.png')\n",
    "\n",
    "# Plot and save Precision-Recall Curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % prc_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('alexnet_precision_recall_curve_synthetic_PCA.png')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
