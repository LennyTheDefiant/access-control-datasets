{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                             roc_auc_score, precision_recall_curve, auc, confusion_matrix)\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/kaggle/input/concatenated-pca/concatenated_data_synthethic_PCA.csv', header=None, delimiter=',')\n",
    "\n",
    "# Drop columns that are not features\n",
    "X = df.iloc[:,:]  # Features\n",
    "y = df.iloc[:, -4:]   # Target variable\n",
    "\n",
    "# Combine the multilabel targets into a single multiclass target\n",
    "unique_rows = np.unique(y, axis=0).tolist()\n",
    "y['combined'] = y.apply(lambda row: unique_rows.index(list(row)), axis=1)\n",
    "y = y['combined'].astype(int)\n",
    "\n",
    "# Setup KFold for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results from each fold\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "roc_aucs = []\n",
    "prc_aucs = []\n",
    "tprs = []\n",
    "fprs = []\n",
    "\n",
    "all_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "mean_roc_auc = 0.0\n",
    "\n",
    "all_recall = np.linspace(0, 1, 100)\n",
    "mean_precision = 0.0\n",
    "mean_prc_auc = 0.0\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_scaled, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # MLP\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=30, activation='relu', solver='adam', random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions, average='macro'))\n",
    "    precisions.append(precision_score(y_test, predictions, average='macro'))\n",
    "    recalls.append(recall_score(y_test, predictions, average='macro'))\n",
    "    roc_aucs.append(roc_auc_score(y_test, proba, multi_class='ovr', average='macro'))\n",
    "\n",
    "    # Calculate TPR and FPR\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    tprs.append(np.mean(tpr))\n",
    "    fprs.append(np.mean(fpr))\n",
    "    \n",
    "    # Compute ROC curve for each class\n",
    "    for i in range(len(unique_rows)):\n",
    "        fpr, tpr, _ = roc_curve(y_test == i, proba[:, i])\n",
    "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        mean_roc_auc += auc(fpr, tpr)\n",
    "        \n",
    "    # Compute precision-recall curve for each class\n",
    "    for i in range(len(unique_rows)):\n",
    "        precision, recall, _ = precision_recall_curve(y_test == i, proba[:, i])\n",
    "        mean_precision += np.interp(all_recall, recall[::-1], precision[::-1])\n",
    "        mean_precision[0] = 1.0\n",
    "        mean_prc_auc += auc(recall, precision)\n",
    "        \n",
    "# Calculate average of all metrics\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_f1 = np.mean(f1_scores)\n",
    "average_precision = np.mean(precisions)\n",
    "average_recall = np.mean(recalls)\n",
    "average_roc_auc = np.mean(roc_aucs)\n",
    "average_tpr = np.mean(tprs)\n",
    "average_fpr = np.mean(fprs)\n",
    "\n",
    "# Print summary of metrics\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average Precision: {average_precision:.2f}\")\n",
    "print(f\"Average Recall: {average_recall:.2f}\")\n",
    "print(f\"Average ROC AUC: {average_roc_auc:.2f}\")\n",
    "print(f\"Average TPR: {average_tpr:.2f}\")\n",
    "print(f\"Average FPR: {average_fpr:.2f}\")\n",
    "\n",
    "# Normalize mean_tpr and mean_roc_auc\n",
    "mean_tpr /= (n_splits * len(unique_rows))\n",
    "mean_roc_auc /= (n_splits * len(unique_rows))\n",
    "\n",
    "# Plot micro-average ROC curve\n",
    "plt.figure()\n",
    "plt.plot(all_fpr, mean_tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % mean_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) curve (micro-average)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('MLP_roc_curve_synthetic_PCA.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Normalize mean_precision and mean_prc_auc\n",
    "mean_precision /= (n_splits * len(unique_rows))\n",
    "mean_prc_auc /= (n_splits * len(unique_rows))\n",
    "\n",
    "# Plot micro-average precision-recall curve\n",
    "plt.figure()\n",
    "plt.plot(all_recall, mean_precision, color='darkorange', lw=2, label='PR curve (area = %0.2f)' % mean_prc_auc)\n",
    "plt.plot([0, 1], [1, 0], color='black', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve (micro-average)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('MLP_prc_curve_synthetic_PCA.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Save metrics to a JSON file\n",
    "metrics = {\n",
    "    'Average Accuracy': average_accuracy,\n",
    "    'Average F1 Score': average_f1,\n",
    "    'Average Precision': average_precision,\n",
    "    'Average Recall': average_recall,\n",
    "    'Average ROC AUC': average_roc_auc,\n",
    "    'Average TPR': average_tpr,\n",
    "    'Average FPR': average_fpr\n",
    "}\n",
    "with open('MLP_evaluation_metrics_synthetic_PCA.json', 'w') as file:\n",
    "    json.dump(metrics, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
