{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.models import load_model\n",
    "from pandas import read_csv\n",
    "from numpy import genfromtxt\n",
    "debug = True\n",
    "\n",
    "\n",
    "f1_scores = []\n",
    "fpr = []\n",
    "tpr = []\n",
    "            \n",
    "print('Original Dataset')\n",
    "amazon_s = genfromtxt('/kaggle/input/amazon-kaggle-smote/Amazon_Kaggle_SMOTE.csv', delimiter=',')\n",
    "ids1 = amazon_s[:,:2]\n",
    "meta1 = amazon_s[:,2:-1]\n",
    "output1 = amazon_s[:,-1:]\n",
    "print(\"done\")\n",
    "print(ids1)\n",
    "print(meta1.shape)\n",
    "print(output1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(meta1 ,output1,test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def create_tabular_vgg16(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Flatten the input\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Adjust output layer based on your task (e.g., binary classification)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape based on the number of features in your tabular data\n",
    "input_shape = (9,)  # Replace num_features with the actual number of features in your dataset\n",
    "\n",
    "# Create tabular VGG16 model\n",
    "model = create_tabular_vgg16(input_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 60 epochs seems to be the sweet spot; tried 10, 20, 30, and 100\n",
    "# batch size seems better at 16; tried 8, 32, 64 \n",
    "model.fit(x_train, y_train, epochs=60, batch_size=16, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test, verbose = 0) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n",
    "outputFileName = 'vgg16_nosmote'\n",
    "DIR_ASSETS = 'results/'\n",
    "PATH_MODEL = DIR_ASSETS + outputFileName + '.hdf5'\n",
    "\n",
    "if debug:\n",
    "  print('Saving trained vgg16 to {}.'.format(PATH_MODEL))\n",
    "if not os.path.isdir(DIR_ASSETS):\n",
    "    os.mkdir(DIR_ASSETS)\n",
    "model.save(PATH_MODEL)\n",
    "\n",
    "# measure True Positive/ Negative, False Positive/ Negative\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Flatten the predicted and actual values\n",
    "y_pred_flat = y_pred.flatten()\n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "# Convert to binary classification (example)\n",
    "# Adjust this step according to your specific problem\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred_flat > threshold).astype(int)\n",
    "\n",
    "# Reshape y_test_flat to match the shape of y_pred_flat\n",
    "y_test_binary = (y_test_flat[:len(y_pred_binary)] > threshold).astype(int)\n",
    "#y_pred = to_categorical(y_pred)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary, average='weighted')\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "cm = confusion_matrix(y_test_binary.ravel(), y_test_binary.ravel())\n",
    "#np.savetxt('cm.txt', cm, delimiter=',', fmt='%f')\n",
    "#f1 = f1_score(y_test.ravel(), y_pred.ravel())\n",
    "tpr_value = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "fpr_value = cm[0, 1] / (cm[0, 1] + cm[0, 0])\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision Score:\", precision)\n",
    "print(\"Recall Score:\", recall)\n",
    "print(\"TPR Score:\", tpr_value)\n",
    "print(\"FPR Score:\", fpr_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "precision_a, recall_a, threshold = precision_recall_curve(y_test_binary, y_pred_binary)\n",
    "prc_auc = auc(recall_a, precision_a)\n",
    "print(\"Area Under the PR Curve score: \", prc_auc)\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test_binary, y_pred_binary)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area Under the ROC Curve score: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming tpr and fpr are NumPy arrays\n",
    "print(\"Type of f1:\", type(f1))\n",
    "print(\"Type of precision:\", type(precision))\n",
    "print(\"Type of recall:\", type(recall))\n",
    "print(\"Type of tpr_value:\", type(tpr_value))\n",
    "print(\"Type of fpr_value:\", type(fpr_value))\n",
    "print(\"Type of roc_auc:\", type(roc_auc))\n",
    "print(\"Type of prc_auc:\", type(prc_auc))\n",
    "print(\"Type of tpr:\", type(tpr))\n",
    "print(\"Type of fpr:\", type(fpr))\n",
    "\n",
    "precision_list = precision_a.tolist()\n",
    "recall_list = recall_a.tolist()\n",
    "tpr_list = tpr.tolist()\n",
    "fpr_list = fpr.tolist()\n",
    "\n",
    "#print(f1.type())\n",
    "\n",
    "data = {\n",
    "    \"Average F1 Score\": f1, \n",
    "    \"Average Precision\": precision, \n",
    "    \"Average Recall\": recall, \n",
    "    \"Average True Positive Rate\": tpr_value, \n",
    "    \"Average False Positive Rate\": fpr_value, \n",
    "    \"Average ROC AUC\": roc_auc, \n",
    "    \"Average PRC AUC\": prc_auc,\n",
    "    \"TPR Array\" : tpr_list,\n",
    "    \"FPR Array\" : fpr_list,\n",
    "    \"Precision Array\": precision_list, \n",
    "    \"Recall Array\": recall_list\n",
    "}\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/kaggle/working/VGG-16 - Real-Life - SMOTE.json\"\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC curve\n",
    "#plt.figure()\n",
    "#plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "#plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "#plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('/kaggle/working/roc_curve.png')\n",
    "#plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "#plt.figure()\n",
    "#plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (AUC = %0.2f)' % average_precision)\n",
    "#plt.xlabel('Recall')\n",
    "#plt.ylabel('Precision')\n",
    "#plt.title('Precision-Recall Curve')\n",
    "#plt.legend(loc=\"lower left\")\n",
    "#plt.savefig('/kaggle/working/precision_recall_curve.png')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
