{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.models import load_model\n",
    "debug = True\n",
    "\n",
    "\n",
    "f1_scores = []\n",
    "fpr = []\n",
    "tpr = []\n",
    "\n",
    " #Load dataset from CSV\n",
    "dataFileName = '/kaggle/input/synthetic-dataset-with-pca/Synthetic_Dataset_with_PCA.csv'  # Update this path to your CSV file\n",
    "# Load the dataset\n",
    "raw_dataset = np.genfromtxt(dataFileName, delimiter=',', dtype=float)  # Update delimiter if necessary\n",
    "PCA = raw_dataset[:] # Skip UID RID, adjust if your CSV structure is different\n",
    "PCA = PCA[1:]\n",
    "\n",
    " #Load dataset from CSV\n",
    "dataFileName = '/kaggle/input/combined-dataset-before-smote/output-2.csv'  # Update this path to your CSV file\n",
    "# Load the dataset\n",
    "synthetic_dataset = np.genfromtxt(dataFileName, delimiter=',', dtype=float)  # Update delimiter if necessary\n",
    "\n",
    "outputs = synthetic_dataset[:, -4:]\n",
    "print(PCA.shape)\n",
    "print(outputs.shape)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(PCA, outputs, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 16  # trained all networks with batch_size=16\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Adjust input_dim according to the number of features in your data\n",
    "model.add(Dense(64, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'sigmoid' for binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 60 epochs seems to be the sweet spot; tried 10, 20, 30, and 100\n",
    "# batch size seems better at 16; tried 8, 32, 64 \n",
    "model.fit(x_train, y_train, epochs=60, batch_size=16, validation_data=(x_test, y_test))\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test, verbose = 0) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])   \n",
    "\n",
    "outputFileName = 'vgg16_PCA'\n",
    "DIR_ASSETS = 'results/'\n",
    "PATH_MODEL = DIR_ASSETS + outputFileName + '.hdf5'\n",
    "\n",
    "if debug:\n",
    "  print('Saving trained vgg16 to {}.'.format(PATH_MODEL))\n",
    "if not os.path.isdir(DIR_ASSETS):\n",
    "    os.mkdir(DIR_ASSETS)\n",
    "model.save(PATH_MODEL)\n",
    "\n",
    "# measure True Positive/ Negative, False Positive/ Negative\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Flatten the predicted and actual values\n",
    "y_pred_flat = y_pred.flatten()\n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "# Convert to binary classification (example)\n",
    "# Adjust this step according to your specific problem\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred_flat > threshold).astype(int)\n",
    "\n",
    "# Reshape y_test_flat to match the shape of y_pred_flat\n",
    "y_test_binary = (y_test_flat[:len(y_pred_binary)] > threshold).astype(int)\n",
    "#y_pred = to_categorical(y_pred)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary, average='weighted')\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "cm = confusion_matrix(y_test_binary.ravel(), y_test_binary.ravel())\n",
    "#np.savetxt('cm.txt', cm, delimiter=',', fmt='%f')\n",
    "#f1 = f1_score(y_test.ravel(), y_pred.ravel())\n",
    "tpr_value = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "fpr_value = cm[0, 1] / (cm[0, 1] + cm[0, 0])\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision Score:\", precision)\n",
    "print(\"Recall Score:\", recall)\n",
    "print(\"TPR Score:\", tpr_value)\n",
    "print(\"FPR Score:\", fpr_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "precision_a, recall_a, threshold = precision_recall_curve(y_test_binary, y_pred_binary)\n",
    "prc_auc = auc(recall_a, precision_a)\n",
    "print(\"Area Under the PR Curve score: \", prc_auc)\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test_binary, y_pred_binary)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area Under the ROC Curve score: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming tpr and fpr are NumPy arrays\n",
    "print(\"Type of f1:\", type(f1))\n",
    "print(\"Type of precision:\", type(precision))\n",
    "print(\"Type of recall:\", type(recall))\n",
    "print(\"Type of tpr_value:\", type(tpr_value))\n",
    "print(\"Type of fpr_value:\", type(fpr_value))\n",
    "print(\"Type of roc_auc:\", type(roc_auc))\n",
    "print(\"Type of prc_auc:\", type(prc_auc))\n",
    "print(\"Type of tpr:\", type(tpr))\n",
    "print(\"Type of fpr:\", type(fpr))\n",
    "\n",
    "precision_list = precision_a.tolist()\n",
    "recall_list = recall_a.tolist()\n",
    "tpr_list = tpr.tolist()\n",
    "fpr_list = fpr.tolist()\n",
    "\n",
    "#print(f1.type())\n",
    "\n",
    "data = {\n",
    "    \"Average F1 Score\": f1, \n",
    "    \"Average Precision\": precision, \n",
    "    \"Average Recall\": recall, \n",
    "    \"Average True Positive Rate\": tpr_value, \n",
    "    \"Average False Positive Rate\": fpr_value, \n",
    "    \"Average ROC AUC\": roc_auc, \n",
    "    \"Average PRC AUC\": prc_auc,\n",
    "    \"TPR Array\" : tpr_list,\n",
    "    \"FPR Array\" : fpr_list,\n",
    "    \"Precision Array\": precision_list, \n",
    "    \"Recall Array\": recall_list\n",
    "}\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/kaggle/working/VGG-16 - Synthetic Dataset with PCA.json\"\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
